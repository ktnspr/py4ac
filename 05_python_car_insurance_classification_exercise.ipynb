{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a26c62",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ktnspr/py4ac/blob/main/05_python_car_insurance_classification_exercise.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/ktnspr/py4ac/blob/main/05_python_car_insurance_classification_exercise.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8f00d",
   "metadata": {},
   "source": [
    "# ExerciseÂ â€“ Predicting Carâ€‘Insurance Claims  \n",
    "\n",
    "Welcome to the practical part of our **Visualization and Machine Learning course**.  \n",
    "In this notebook you will guide a small insurance company through the complete **classification workflow**:  \n",
    "* exploratory data analysis (EDA)  \n",
    "* data cleaning and feature engineering  \n",
    "* model training & evaluation  \n",
    "* first steps towards model interpretability  \n",
    "\n",
    "ðŸ’¡ **How to work with the notebook**  \n",
    "* Every upcoming code cell is announced by a markdown block (like this one) that explains what needs to be done.\n",
    "* Feel free to experiment â€“ the dataset is small and reloads in a second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2716d66",
   "metadata": {},
   "source": [
    "## 1Â â€“ Import the required libraries  \n",
    "\n",
    "The next code cell should collect **all library imports** you will need later, including:  \n",
    "\n",
    "* **Pandas** â€“ data handling  \n",
    "* **Seaborn / Matplotlib** â€“ quick visualisations  \n",
    "* **Scikitâ€‘learn** â€“ preprocessing, model selection and metrics  \n",
    "\n",
    "ðŸ‘‰ **Your task:** Make sure the imports are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "# Enable nicer plots inside the notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e88c03",
   "metadata": {},
   "source": [
    "## 2Â â€“ Load the dataset  \n",
    "\n",
    "Our CSV file contains one row per customer and the target column **`OUTCOME`** that indicates whether the customer filed a claim last year.  \n",
    "\n",
    "ðŸ‘‰ **Your task:**  \n",
    "1. Read the CSV (hint: `pd.read_csv`).  \n",
    "2. Show the first five rows â€“ this immediately reveals spelling issues in column names.  \n",
    "3. Print the shape so you know how many observations you have to play with.  \n",
    "4. (Optional) Adjust the path if the dataset is in a different folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f3687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Â â€“ Load data\n",
    "path = kagglehub.dataset_download(\"sagnik1511/car-insurance-data\")\n",
    "print(\"Path to dataset:\", path)\n",
    "df = pd.read_csv(Path(path)/'Car_Insurance_Claim.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c872729",
   "metadata": {},
   "source": [
    "## 3Â â€“ Quick exploratory analysis (EDA)  \n",
    "\n",
    "Before jumping into modelling we **look at the raw data**. This helps us to spot dataâ€‘quality issues and build an intuition for useful features.  \n",
    "\n",
    "ðŸ‘‰ **Your tasks:**  \n",
    "1. **Class balance:** Create a countâ€‘plot of `OUTCOME` â€“ how imbalanced is the target?  Use `sns.countplot`!\n",
    "2. **Outcome by age:** Visualise the distribution of claims across age groups; a stacked or grouped bar chart works well.  \n",
    "3. **Feature correlations:** For **numeric columns** compute a correlation matrix and show it as a heatâ€‘map. Search for highly correlated features that could be removed later. Use the **pandas method** `df.select_dtypes(include=['int64', 'float64'])` for selecting the numerical features only, then use `sns.heatmap`!\n",
    "4. **Creditâ€‘score vs outcome:** A boxâ€‘plot is a quick way to see if lower credit scores correlate with more claims. Use `sns.boxplot`!  \n",
    "\n",
    "*(Tip: keep the figures small â€“ `figsize=(6,4)` is usually enough.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Â â€“ Exploratory plots\n",
    "# --- Class balance, variable OUTCOME\n",
    "...\n",
    "\n",
    "# --- AGE vs OUTCOME\n",
    "...\n",
    "\n",
    "# --- Correlation heatâ€‘map (numeric features only)\n",
    "...\n",
    "\n",
    "# --- CREDIT_SCORE distribution per class\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fcf7a",
   "metadata": {},
   "source": [
    "## 4Â â€“ Separate feature matrix **X** and target **y**  \n",
    "\n",
    "Machineâ€‘learning APIs expect **`X`** (all predictors) and **`y`** (label) as separate objects. We also drop nonâ€‘informative identifier columns such as `ID` and `POSTAL_CODE`.  \n",
    "\n",
    "ðŸ‘‰ **Your tasks:**  \n",
    "1. Assign `df[\"OUTCOME\"]` to `y`.  \n",
    "2. Create `X` by dropping `OUTCOME`, `ID`, and `POSTAL_CODE`.  \n",
    "3. Doubleâ€‘check: `X.shape[0]` should equal `y.shape[0]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a050e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Â â€“ Target and features\n",
    "y = ...\n",
    "X = ...\n",
    "\n",
    "# check that shape of X and y match\n",
    "assert X.shape[0] == y.shape[0], \"Mismatch between number of samples in X and y\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96069b85",
   "metadata": {},
   "source": [
    "## 5 â€“ Detect categorical vs numerical columns  \n",
    "\n",
    "We have to treat **categorical** and **numerical** features differently (dummyâ€‘encoding vs. scaling).  \n",
    "The **pandas method** `select_dtypes` helps us identify the relevant columns. Since you only want the column names, use `.columns.tolist()`.\n",
    "\n",
    "ðŸ‘‰ **Your tasks:**  \n",
    "1. Build a list `cat_cols` containing all `object` columns.  \n",
    "2. Build a list `num_cols` containing `int64` and `float64` columns.  \n",
    "3. Print both lists â€“ are the column names plausible?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Â â€“ Column type detection\n",
    "cat_cols = ...\n",
    "num_cols = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b442a",
   "metadata": {},
   "source": [
    "## 6Â â€“ Preâ€‘processing pipeline  \n",
    "\n",
    "We perform three independent steps:  \n",
    "1. **Oneâ€‘hot encoding** for categoricals (`pd.get_dummies`). Use the previously created list of categorical columns `columns=cat_cols` and use `drop_first=True` to avoid perfect multicollinearity.  \n",
    "2. **Missingâ€‘value imputation** â€“ replacing NA by the **most frequent category** is a decent default for this dataset. Use `SimpleImputer` with `strategy=\"most_frequent\"`!\n",
    "3. **Standardization** of numerical columns so that gradientâ€‘based learners (like an MLP) converge faster. Use `StandardScaler` with the `fit_transform`-method. Use the previously created list of numerical columns, `num_cols`.\n",
    "\n",
    "ðŸ‘‰ **Your tasks:** Implement each step in the given order. After scaling, print `X_encoded.head()` to verify the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f59975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6Â â€“ Oneâ€‘hot encoding of all categorical features\n",
    "X_encoded = ...\n",
    "\n",
    "# 6b â€“ Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "X_encoded = ...\n",
    "\n",
    "# 6c â€“ Standardise numerical features\n",
    "scaler = StandardScaler()  #!\n",
    "X_encoded[num_cols] = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4f09b",
   "metadata": {},
   "source": [
    "## 7Â â€“ Train / test split  \n",
    "\n",
    "To evaluate our models we keep **20â€¯%** of the data as an **unseen holdâ€‘out set**. Use `train_test_split` with a fixed `random_state` so that your results are reproducible.  \n",
    "\n",
    "ðŸ‘‰ **Your tasks:**  \n",
    "* Call `train_test_split` with `test_size=0.2` and `random_state=42`.  \n",
    "* Examine the class ratio in `y_train` and `y_test` â€“ do they match the original distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7Â â€“ Train/test split\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Class ratios in train and test sets\n",
    "print(\"Class ratios in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass ratios in test set:\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25b5be",
   "metadata": {},
   "source": [
    "## 8Â â€“ Configure the models  \n",
    "\n",
    "We will compare three real classifiers against a **baseline DummyClassifier**:  \n",
    "\n",
    "| Model | Key hyperâ€‘parameters | Strength | Weakness |\n",
    "|-------|----------------------|----------|----------|\n",
    "| NaÃ¯veÂ Bayes | default | works on small data | assumes feature independence |\n",
    "| DecisionÂ Tree | `max_depth=5` | interpretable & nonâ€‘linear | prone to overfit | \n",
    "| MLP | one hidden layer with 50 neurons | captures complex patterns | needs scaling + more compute |\n",
    "| dummy | `strategy=\"most_frequent\"` | good for comparison | well... |\n",
    "\n",
    "ðŸ‘‰ **Your tasks:** Instantiate all four models in a dictionary called `models`. Feel free to tweak `max_depth`, `hidden_layer_sizes`, or `max_iter` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02495559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8Â â€“ Model dictionary\n",
    "models = {\n",
    "    \"Naive Bayes\": ... # GaussianNB(),\n",
    "    \"Decision Tree\": ... # DecisionTreeClassifier(),\n",
    "    \"MLP\": ... # MLPClassifier(),\n",
    "    \"Dummy\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3e144",
   "metadata": {},
   "source": [
    "## 9Â â€“ Train, predict, collect metrics  \n",
    "\n",
    "For each model:  \n",
    "1. Fit on the training data: `model.fit`\n",
    "2. Predict the test set: `model.predict`\n",
    "3. Store **accuracy** (`accuracy_score`), as well as the macroâ€‘averages of **precision (`precision_score`), recall (`recall_score`), F1 (`f1_score`)** in a list of dictionaries. Always use `average=\"macro\"`.\n",
    "\n",
    "ðŸ‘‰ **Your tasks:** Fill the loop and ensure each metric lands in the `results` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c778aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9Â â€“ Loop over models\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    ... # fit\n",
    "    ... # predict\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision (macro)\": precision_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"Recall (macro)\": recall_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"F1-Score (macro)\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f830646",
   "metadata": {},
   "source": [
    "## 10Â â€“ Compare the results  \n",
    "\n",
    "Turn `results` into a DataFrame and round to three decimals so differences are visible. The Dummy baseline shows how far a trivial guess gets us â€“ our models should beat it.  \n",
    "\n",
    "ðŸ‘‰ **Your tasks:** Just display the DataFrame. You might consider using `results_df.plot.bar(x=\"Model\", y=\"Accuracy\")` for a quick visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10Â â€“ Results table\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a3d6e",
   "metadata": {},
   "source": [
    "## 11Â â€“ Inspect Decisionâ€‘Tree feature importance  \n",
    "\n",
    "Treeâ€‘based models come with builtâ€‘in feature importance scores that reflect the average information gain contributed by each split.  \n",
    "\n",
    "ðŸ‘‰ **Your tasks:**  \n",
    "1. Extract `feature_importances_` from the trained tree.  \n",
    "2. Build a DataFrame with columns `Feature` and `Importance`.  \n",
    "3. Plot a horizontal barâ€‘chart (lowest importance on top) so that the most influential features are easy to spot.  \n",
    "4. Reflect: Does the ranking align with your domain intuition from sectionÂ 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a64dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11Â â€“ Feature importance plot\n",
    "if isinstance(models[\"Decision Tree\"], DecisionTreeClassifier):\n",
    "    feature_importances = models[\"Decision Tree\"].feature_importances_\n",
    "    importance_df = (\n",
    "        pd.DataFrame({\n",
    "            \"Feature\": X_encoded.columns,\n",
    "            \"Importance\": feature_importances\n",
    "        })\n",
    "        .sort_values(by=\"Importance\", ascending=True)\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color='skyblue')\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(\"Feature importance â€“ Decision Tree\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
