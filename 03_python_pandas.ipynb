{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Actuaries Part 3\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ktnspr/py4ac/blob/main/03_python_pandas.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/ktnspr/py4ac/blob/main/03_python_pandas.ipynb)\n",
    "## Agenda\n",
    "In this notebook, we will cover:\n",
    "- Introduction to Pandas\n",
    "- Series and DataFrames\n",
    "- Data access and inspection\n",
    "- Data manipulation and cleaning\n",
    "- Reading files\n",
    "\n",
    "# Introduction to Pandas\n",
    "\n",
    "**Pandas** is one of the most popular libraries for data analysis and manipulation in Python. It was specifically designed to work efficiently and easily with structured data, similar to how spreadsheets like Excel operate.\n",
    "\n",
    "Pandas provides two main structures for handling data:\n",
    "- **Series**: A one-dimensional data structure, similar to a list or an array.\n",
    "- **DataFrame**: A two-dimensional data structure, comparable to a table in a database or an Excel spreadsheet.\n",
    "\n",
    "### Why Pandas?\n",
    "Pandas is particularly useful when it comes to reading, processing, and analyzing large amounts of data. Typical tasks that Pandas simplifies include:\n",
    "- Reading data from various formats (e.g., CSV, Excel, SQL databases).\n",
    "- Efficient inspection and analysis of data.\n",
    "- Data cleaning and manipulation (e.g., handling missing values, renaming columns).\n",
    "- Transformation and aggregation of data for further analysis or visualizations.\n",
    "\n",
    "Due to its broad functionality and ease of use, Pandas has become a standard tool for data analysis, especially in the fields of data science, financial analysis, and insurance.\n",
    "\n",
    "In the following sections, we will look at the key features of Pandas and learn how to efficiently read, analyze, and clean data.\n",
    "\n",
    "### Criticisms of Pandas\n",
    "\n",
    "Although Pandas is a powerful library, there are also some criticisms to keep in mind:\n",
    "\n",
    "1. **Memory and computational overhead**: Pandas is memory-intensive as it loads data into memory. With very large datasets, this can lead to performance issues. For large datasets that do not fit into RAM, alternative tools such as **Dask**, **PySpark**, or **Apache Arrow** should be considered, which are optimized for distributed processing.\n",
    "\n",
    "2. **Complexity with large data pipelines**: For smaller projects and analyses, Pandas is very efficient. However, with larger data pipelines or complex data analyses, the code can become unwieldy and hard to maintain. In particular, dealing with many nested Pandas functions can make the code difficult to understand.\n",
    "\n",
    "Despite these limitations, Pandas remains a powerful tool for most data analyses, especially with medium-sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work with Pandas, it obviously needs to be imported\n",
    "import pandas as pd\n",
    "# Typically, NumPy is also needed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series\n",
    "\n",
    "A **Pandas Series** is a one-dimensional data structure that is comparable to a list or an array. It consists of a series of values, each associated with an index. Pandas Series can contain various data types such as integers, floating-point numbers, or strings.\n",
    "\n",
    "### Properties of a Series:\n",
    "- **Index**: Each Series has an associated index, which by default starts at 0 and increments, but can also be explicitly set (e.g., dates).\n",
    "- **Homogeneous Data**: All elements of a Series have the same data type (like in arrays).\n",
    "\n",
    "Series are useful when working with one-dimensional data, such as a list of damage amounts in an insurance portfolio.\n",
    "\n",
    "## Example 1: Damage History Over Several Years\n",
    "\n",
    "In this example, we will store the annual damage history of an insurance company as a Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claims history over five years\n",
    "years = ['2018', '2019', '2020', '2021', '2022']\n",
    "claims_history = pd.Series([10000, 15000, 13000, 12000, 11000], index=years)\n",
    "print(claims_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example is (synthetic) stock trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the simulation\n",
    "np.random.seed(42)\n",
    "days = pd.date_range(start='2023-01-01', periods=365, freq='D')\n",
    "starting_price = 100  # Initial stock price\n",
    "volatility = 0.02  # Stock volatility (2%)\n",
    "expected_return = 0.001  # Expected daily return (0.1%)\n",
    "\n",
    "# Generate daily changes based on Brownian motion\n",
    "changes = np.random.normal(loc=expected_return, scale=volatility, size=len(days))\n",
    "\n",
    "# Calculate stock prices using cumulative sum\n",
    "prices = starting_price * np.exp(np.cumsum(changes))\n",
    "\n",
    "# Create the Pandas Series\n",
    "stock_series = pd.Series(prices, index=days)\n",
    "\n",
    "# Display the first few days of the stock price series\n",
    "print(stock_series.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "A **DataFrame** is the core of Pandas and represents a two-dimensional, table-like data structure that contains columns and rows. One can think of a DataFrame as a collection of Pandas Series that share a common structure. Each column is assigned a unique name, and each row can be referenced by an index.\n",
    "\n",
    "### Properties of a DataFrame:\n",
    "- **Rows and Columns**: A DataFrame consists of rows and columns, with each column potentially having its own data type (e.g., one column with numeric values and another column with strings).\n",
    "- **Flexible Indexing**: You can access both rows and columns using the index or column names.\n",
    "- **Data Sources**: DataFrames can be created from various data sources, such as CSV files, Excel files, SQL databases, or even from dictionaries and lists in Python.\n",
    "\n",
    "## Example 1: Creating a DataFrame for Insurance Data\n",
    "\n",
    "In this example, we will create a DataFrame that contains some basic information about insurance policies, such as the policy ID, the insured person, the premium amount, and the number of claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "# Define the data\n",
    "data = {\n",
    "    'Policy_ID': pd.Series([101, 102, 103, 104, 105, 106, 107, 108, 109, 110], dtype='int64'),\n",
    "    'Insured_Person': pd.Series(['John Smith', 'Emma Johnson', 'Michael Brown', 'Olivia Davis', 'William Miller',\n",
    "                          'Sophia Wilson', 'James Moore', 'Isabella Taylor', 'Lucas Anderson', 'Mia Thomas'], dtype='string'),\n",
    "    'Age': pd.Series([18, 47, 49, 14, 15, 31, 1, 54, 3, 16], dtype='int64'),\n",
    "    'Premium_Amount': pd.Series([500, 600, 550, 620, 480, 700, 520, 610, 580, 540], dtype='float64'),\n",
    "    'Claims': pd.Series([2, 1, 0, 3, 2, 1, 1, 0, 4, 2], dtype='int64'),\n",
    "    'Policy_Start': pd.to_datetime(['2019-01-15', '2020-05-20', '2021-03-10', '2018-11-22', \n",
    "                                    '2019-07-01', '2020-02-17', '2021-09-09', '2022-01-01', \n",
    "                                    '2019-12-25', '2020-06-14'])\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "insurance_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "insurance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, the DataFrame does not have an explicit index. Therefore, the column ranges from 0 to 9 at the front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of columns\n",
    "insurance_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the translation:\n",
    "\n",
    "Another example of time-dependent (synthetic) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the simulation\n",
    "np.random.seed(42)\n",
    "days = pd.date_range(start='2023-01-01', periods=365, freq='D')\n",
    "\n",
    "# Stock parameters for different companies\n",
    "companies = {\n",
    "    'Apple': {'start_price': 150, 'volatility': 0.02, 'return': 0.001},\n",
    "    'Nvidia': {'start_price': 220, 'volatility': 0.025, 'return': 0.0012},\n",
    "    'Google': {'start_price': 180, 'volatility': 0.018, 'return': 0.0008},\n",
    "    'SAP': {'start_price': 120, 'volatility': 0.015, 'return': 0.0009},\n",
    "    'Microsoft': {'start_price': 250, 'volatility': 0.02, 'return': 0.0011},\n",
    "    'Tesla': {'start_price': 200, 'volatility': 0.03, 'return': 0.0015},\n",
    "}\n",
    "\n",
    "# Create a DataFrame to store all stock trajectories\n",
    "stock_prices = pd.DataFrame(index=days)\n",
    "\n",
    "# Simulate stock prices for each company\n",
    "for company, params in companies.items():\n",
    "    start_price = params['start_price']\n",
    "    volatility = params['volatility']\n",
    "    expected_return = params['return']\n",
    "    \n",
    "    # Generate daily changes based on Brownian motion\n",
    "    changes = np.random.normal(loc=expected_return, scale=volatility, size=len(days))\n",
    "    \n",
    "    # Calculate stock prices with cumulative sum\n",
    "    prices = start_price * np.exp(np.cumsum(changes))\n",
    "    \n",
    "    # Add the simulation as a series in the DataFrame\n",
    "    stock_prices[company] = prices\n",
    "\n",
    "# Output the first 20 days of the simulated stock prices\n",
    "stock_prices.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two nice example datasets `insurance_df` and `stock_prices`. Let's see what we can do with them.\n",
    "\n",
    "# Pandas Standard Functions\n",
    "\n",
    "Pandas offers a range of helpful standard functions to quickly and efficiently inspect data. Here are some of the most important ones:\n",
    "`head()` and `tail()`\n",
    "\n",
    "With `head()` and `tail()`, you can display the first or last entries of a DataFrame or a Series. This is useful for getting a quick overview of the structure of the data. We have already seen examples (from `head()`) above.\n",
    "\n",
    "More interesting for getting a first impression of a dataset are the functions `describe()` and `info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important information includes, for example, `columns`, `index`, or `shape`. (Note that these are not functions but attributes of the DataFrame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access in Pandas\n",
    "\n",
    "In Pandas, there are various methods to access data in a DataFrame. This includes accessing columns, rows, or specific elements using `loc` and `iloc`.\n",
    "\n",
    "### Accessing Columns\n",
    "\n",
    "A column of a DataFrame can be easily accessed using the column name. There are two ways to do this:\n",
    "\n",
    "1. Access using dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_df.Insured_Person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. With column names in square brackets (more common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_df['Insured_Person']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Rows\n",
    "\n",
    "`loc[]` â€“ Access by Labels (Index-based)\n",
    "\n",
    "With 'loc[]', you can access rows and columns based on the labels. This method uses explicit index names or column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices.loc['01-01-2023'] #Watch out for the date format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iloc[]` - Integer-based Access\n",
    "\n",
    "With `iloc[]`, you access rows and columns based on their position in the DataFrame. This works similarly to a list in Python, where the indices are 0-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Columns and Rows + Slicing\n",
    "You can also access columns and rows simultaneously, plus select only parts of the data (*slicing*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the first 5 days of the stock prices for Apple and Nvidia\n",
    "stock_prices.loc['2023-01-01':'2023-01-05', ['Apple', 'Nvidia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a specific value in the DataFrame\n",
    "insurance_df.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices.loc['2023-12-31', 'Microsoft']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Access\n",
    "\n",
    "In addition to accessing data via column names or indices, you can also access data based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all contracts starting in May\n",
    "insurance_df[insurance_df['Policy_Start'].dt.month == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only contracts with a premium amount greater than 600\n",
    "insurance_df[insurance_df['Premium_Amount'] > 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access is given by a boolean mask\n",
    "insurance_df['Premium_Amount'] > 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional access is very powerful; however, it can also become very complex very quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_prices[\n",
    "    (stock_prices.index.month == 5) &  # Filter for May\n",
    "    ((stock_prices['Nvidia'] > 230) |   # Nvidia > 230 or\n",
    "    (stock_prices['Apple'] > 140)) &    # Apple > 155\n",
    "    (stock_prices['Google'] > 185) &   # Google > 185\n",
    "    (stock_prices['Tesla'] < 260)      # Tesla < 220\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching Data\n",
    "\n",
    "In data analysis, it is often necessary to expand existing datasets by adding new columns. This can be done, for example, through calculations, importing additional data, or applying functions. In Pandas, there is a straightforward way to create new columns and fill them with data.\n",
    "\n",
    "Typical use cases include:\n",
    "\n",
    "- Calculating new values based on existing columns (e.g., risk assessments, premium calculations)\n",
    "- Adding external data sources (e.g., inflation, exchange rates)\n",
    "- Creating categories or groupings (e.g., age divisions, damage classes)\n",
    "\n",
    "New columns can be created in various ways, such as through simple assignment, using calculations, or assigning a constant or calculated series of values.\n",
    "\n",
    "In the following examples, we will look at how to add new columns to an existing DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with the contract duration in years\n",
    "insurance_df['Duration'] = (pd.to_datetime('today') - insurance_df['Policy_Start']).dt.days / 365\n",
    "insurance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute performance relative to the starting value for Apple stock\n",
    "stock_price_apple = pd.DataFrame()\n",
    "stock_price_apple['Price'] = stock_prices['Apple']  \n",
    "start_value_apple = stock_price_apple['Price'].iloc[0]  # Starting value of the stock\n",
    "print(f'Simulated starting value of Apple stock: {start_value_apple}')\n",
    "stock_price_apple['AbsPerformance'] = stock_price_apple['Price'] - start_value_apple\n",
    "\n",
    "# Display the last rows including the new column\n",
    "print(stock_price_apple.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to simple calculations, there are also Pandas standard functions for many use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the relative performance of Apple stock\n",
    "stock_price_apple['RelPerformance'] = stock_price_apple['Price'].pct_change()\n",
    "stock_price_apple.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Data\n",
    "If you want to delete a column or row, you can do so by:\n",
    "\n",
    "- **Deleting Rows**: You can remove specific rows using `drop(index)`. Here, you specify the index of the rows that should be deleted.\n",
    "- **Deleting Columns**: You can remove specific columns using `drop(column_name, axis=1)`.\n",
    "\n",
    "Inplace Option: If you want to apply the changes directly to the existing DataFrame, you can use `inplace=True` to avoid creating a copy of the DataFrame.\n",
    "\n",
    "**Warning**: `drop()` does not allow reverting to the original data once the rows or columns have been deleted, unless a copy was created beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a column\n",
    "stock_price_apple.drop(['RelPerformance'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop returns a new dataframe without the deleted data. Without `inplace=True`, the original dataframe does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the third an fourth row\n",
    "stock_price_apple.drop(['2023-01-03', '2023-01-04'], inplace=True) # Watch the index is a date\n",
    "stock_price_apple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Working with Damage Data\n",
    "\n",
    "In this task, you will perform various operations using a sample dataset of damage data. We will create the data using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating example data\n",
    "data = {\n",
    "    'Claim_ID': range(1, 16),\n",
    "    'Policy_Number': [f'PN{str(i).zfill(5)}' for i in np.random.randint(0, 10000, 15)],  # Random policy numbers\n",
    "    'Claim_Amount': np.random.randint(1000, 5000, size=15),  # Random claim amounts between 1000 and 5000\n",
    "    'Claim_Date': pd.date_range(start='2023-01-01', periods=15, freq='D'),\n",
    "    'Policy_Start': pd.date_range(start='2021-01-01', periods=15, freq='D'),\n",
    "    'Region': [\n",
    "        'North', 'East', 'South', 'West', 'North', \n",
    "        'East', 'South', 'West', 'North', 'East', \n",
    "        'South', 'West', 'North', 'East', 'South'\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. **Create DataFrame**: Create a Pandas DataFrame from the data provided above.\n",
    "\n",
    "2. **View Data**: Use the `head()` and `describe()` methods to get an overview of the dataset. What do you notice?\n",
    "\n",
    "3. **Filter Data**: Filter the claims that have a claim amount of more than 2000. Which policyholders are affected?\n",
    "\n",
    "4. **Determine Contract Duration**: Add a new column `Contract Duration` that indicates the duration of the contract in days until the date of the claim.\n",
    "\n",
    "5. **Calculate Average Claim Amount**: Calculate the average claim amount of the claims.\n",
    "\n",
    "6. **Group Claim Amounts**: Group the claim amounts by region.\n",
    "\n",
    "## Note\n",
    "\n",
    "You can use Pandas functions to solve the tasks mentioned above. Please note that we have not yet discussed the approach for task 6. Take a look at the documentation for `mean()` and `groupby()`. ([Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby))\n",
    "\n",
    "You can obtain the number of different values in the *Region* column using the `value_counts()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Data and Aggregation Functions in Pandas\n",
    "\n",
    "In data analysis, it is often necessary to group data in order to calculate aggregated statistics. Pandas provides powerful functions to group data and perform various aggregation operations.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "- **Grouping**: Data is grouped using the `groupby()` method. This method splits the data into groups based on the values of one or more columns.\n",
    "- **Aggregation**: After grouping, various aggregation functions can be applied to calculate statistical measures. Commonly used aggregation functions include:\n",
    "  - `mean()`: Calculates the average.\n",
    "  - `sum()`: Calculates the sum.\n",
    "  - `count()`: Counts the number of elements.\n",
    "  - `max()`: Determines the maximum value.\n",
    "  - `min()`: Determines the minimum value.\n",
    "\n",
    "## Example: Aggregating Damage Data\n",
    "\n",
    "Suppose we have a DataFrame `damage_data` with damage information, which contains a column for the region and one for the amount of damage. We can calculate the average damage per region as follows:\n",
    "\n",
    "```python\n",
    "average_damage = damage_data.groupby('Region')['Damage Amount'].mean()\n",
    "print(average_damage)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivoting with Pandas\n",
    "\n",
    "Pivoting in Pandas allows you to transform data into a clear table, similar to pivot tables in Excel. With the `pivot()` function, you can restructure data based on key columns (such as categories, regions, or time indicators). One column is used for the values you want to aggregate or display, while other columns are used as the index or column headers.\n",
    "\n",
    "## Example of Pivoting:\n",
    "\n",
    "Suppose we want to create an overview of the average damage amount in different regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table showing the mean claim amounts by region\n",
    "claim_pivot = claim_data.pivot_table(values='Claim_Amount', index='Region', aggfunc='mean')\n",
    "claim_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Data in Pandas DataFrames\n",
    "\n",
    "Pandas offers powerful functions for reading data from various file formats and writing DataFrames to files. The most common formats are CSV (Comma-Separated Values) and Excel, but it also supports JSON, HTML, LaTeX, Parquet, SAS, and more.\n",
    "\n",
    "## Reading Data\n",
    "To read data from a CSV file into a Pandas DataFrame, use the `read_csv()` function. Here is an example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Reading a CSV file\n",
    "df_csv = pd.read_csv('path/to/file.csv')\n",
    "\n",
    "# Output the first five rows of the DataFrame\n",
    "print(df_csv.head())\n",
    "```\n",
    "\n",
    "Reading an Excel file is just as simple:\n",
    "```python\n",
    "# Reading an Excel file\n",
    "df_excel = pd.read_excel('path/to/file.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# Output the first five rows of the DataFrame\n",
    "print(df_excel.head())\n",
    "```\n",
    "\n",
    "## Writing Data\n",
    "To write a DataFrame to a CSV file, use the `to_csv()` function. Here is an example:\n",
    "```python\n",
    "# Writing the DataFrame to a CSV file\n",
    "df_csv.to_csv('path/to/new_file.csv', index=False)\n",
    "```\n",
    "\n",
    "Or:\n",
    "```python\n",
    "# Writing the DataFrame to an Excel file\n",
    "df_excel.to_excel('path/to/new_file.xlsx', sheet_name='Sheet1', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Files from Non-Local Sources\n",
    "\n",
    "You can easily read data sources with `pandas` that are not local but rather \"somewhere\" on the internet or on a server. You just need to specify the URL to the location of the file.\n",
    "\n",
    "For example, we can read files from a GitHub repository like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch for the \"raw\" part of the URL\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/DeutscheAktuarvereinigung/Python_fuer_Aktuare/refs/heads/main/data/weather_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "In real datasets, missing data is a common problem. Pandas offers a variety of methods to identify, handle, or clean missing values.\n",
    "\n",
    "### Detecting Missing Data\n",
    "With the function `isnull()` or `isna()`, you can identify which entries are missing in a DataFrame. `isnull()` returns `True` if the value is missing (NaN) and `False` if a valid value is present.\n",
    "\n",
    "Let's take a look at this using our insurance dataset from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far the dataset has no missing values, so we'll create some\n",
    "nan_df = insurance_df.copy()\n",
    "\n",
    "# Randomly insert NaN values (approx. 25% of the data)\n",
    "for col in ['Age', 'Policy_Start', 'Duration']:\n",
    "    nan_df.loc[nan_df.sample(frac=0.25).index, col] = np.nan\n",
    "\n",
    "nan_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Missing Data\n",
    "\n",
    "With `dropna()`, you can remove rows or columns with missing values. You can control whether to consider all missing values (how='all') or only individual missing values (how='any')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = nan_df.dropna()\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Data\n",
    "\n",
    "Instead of removing missing values, we can also replace them with meaningful values. This can be, for example, the mean, median, or a specified value. Use the `fillna()` function for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df['Age'].fillna(nan_df['Age'].mean(), inplace=True)\n",
    "nan_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Now it's your turn. In the *data* folder, you will find two datasets:\n",
    "- car_insurance_claims.csv (Car insurance data)\n",
    "- titanic_train.csv (Titanic passenger data)\n",
    "\n",
    "Source:\n",
    "\n",
    "The car insurance data comes from [Databrick](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4954928053318020/1058911316420443/167703932442645/latest.html). Unfortunately, no further source is provided there.\n",
    "\n",
    "The Titanic data is taken from the [Titanic Machine Learning Challenge](https://www.kaggle.com/c/titanic/data) on Kaggle and contains data from 892 Titanic passengers. The dataset is used in the challenge for training machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks for the Damage Case Dataset\n",
    "\n",
    "1. Get an Overview\n",
    "- **Task**: Read in the dataset and get an overview.\n",
    "  - Use `head()` to view the first 5 rows of the dataset.\n",
    "  - Use `info()` to obtain information about the data types and the number of entries.\n",
    "  - Display a summary of the numerical columns with `describe()`.\n",
    "\n",
    "*Note* The link to the online storage location of the data is:\n",
    "https://raw.githubusercontent.com/DeutscheAktuarvereinigung/Python_fuer_Aktuare/refs/heads/completed/data/car_insurance_claims.csv\n",
    "\n",
    "2. Column Access and Simple Calculations\n",
    "- **Task**: Work with individual columns.\n",
    "  - Show the average amount of the `total_claim_amount`.\n",
    "  - Find the maximum and minimum value for the `policy_annual_premium` column.\n",
    "  - Create a new column `claim_ratio` that calculates the ratio of `total_claim_amount` to `policy_annual_premium`.\n",
    "\n",
    "3. Filtering Data\n",
    "- **Task**: Filter the dataset based on specific conditions.\n",
    "  - Find all cases where the `incident_type` is reported as \"Single Vehicle Collision\".\n",
    "  - Display all incidents that occurred in the state \"NY\" (`incident_state`) and where the claim amount (`total_claim_amount`) is over 10,000.\n",
    "  - Find all incidents where more than 2 witnesses (`witnesses`) were reported.\n",
    "\n",
    "4. Grouping and Aggregating\n",
    "- **Task**: Group data and calculate averages.\n",
    "  - Group the data by `incident_state` and show the average `total_claim_amount` for each state.\n",
    "  - Group the data by `insured_occupation` and display the mean amount of `injury_claim`.\n",
    "  - Find the average `property_claim` by `auto_make`.\n",
    "\n",
    "5. Handling Missing Data\n",
    "- **Task**: Deal with missing data.\n",
    "  - Determine which columns contain missing values (`NaN`).\n",
    "  - Count the missing values in the `authorities_contacted` column.\n",
    "  - Fill the missing values in the `authorities_contacted` column with the value \"Not Contacted\".\n",
    "\n",
    "6. Time-Based Analyses\n",
    "- **Task**: Work with time data.\n",
    "  - Convert the `incident_date` column to date format (`datetime`).\n",
    "  - Find out in which month the most incidents occurred.\n",
    "  - Display all incidents reported between February 15, 2015, and February 28, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: Analyses of the Titanic Dataset\n",
    "\n",
    "*Note* The link to the online storage location of the data is:\n",
    "https://raw.githubusercontent.com/DeutscheAktuarvereinigung/Python_fuer_Aktuare/refs/heads/completed/data/titanic_train.csv\n",
    "\n",
    "1. **Who was the youngest passenger?**  \n",
    "   Determine the age of the youngest passenger on board the Titanic. What else can you find out about this passenger?\n",
    "\n",
    "2. **Survival rate by gender**  \n",
    "   Investigate whether survival on the Titanic was related to the gender of the passengers. Calculate the survival rate for men and women separately. Use the `Survived` and `Sex` columns for this.\n",
    "\n",
    "3. **Survival chances in different classes**  \n",
    "   Did the class in which a passenger traveled influence their chances of survival? Calculate the survival rate for each of the three classes (`Pclass`) and interpret the results.\n",
    "\n",
    "4. **Average ticket prices per class**  \n",
    "   Find out how much passengers paid on average for their tickets in each class. Use the `Pclass` and `Fare` columns to calculate the average ticket prices.\n",
    "\n",
    "5. **Largest families on board**  \n",
    "   Determine how many siblings and parents (columns `SibSp` and `Parch`) the passengers had on board. Who traveled with the largest family?\n",
    "\n",
    "6. **Where did most passengers embark?**  \n",
    "   Investigate which port (`Embarked`) most passengers boarded from.\n",
    "\n",
    "7. **Distribution of age groups**  \n",
    "   Divide the passengers into different age groups (e.g., children, teenagers, adults, seniors) and examine how these groups affect the survival rate.\n",
    "\n",
    "8. **Most valuable ticket on board**  \n",
    "   Find out which was the most expensive ticket (`Fare`) on the Titanic and who purchased it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
